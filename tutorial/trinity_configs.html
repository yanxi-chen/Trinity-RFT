

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Trinity-RFT Configuration &mdash; Trinity-RFT 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=01f34227"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Trinity-RFT Developer Guide" href="trinity_programming_guide.html" />
    <link rel="prev" title="Data processing functionalities" href="example_data_functionalities.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Trinity-RFT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Tutorial</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="example_reasoning_basic.html">A quick example with GSM8k</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_reasoning_advanced.html">Example: off-policy RFT mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_multi_turn.html">Example: Multi-Turn RFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_dpo.html">Example: Run DPO on Human-Like-DPO-Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_data_functionalities.html">Data processing functionalities</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Trinity-RFT Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#global-config">Global Config</a></li>
<li class="toctree-l2"><a class="reference internal" href="#monitor">Monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-processing">Data Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cluster">Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="#buffer">Buffer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#explorer">Explorer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#synchronizer">Synchronizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#trainer">Trainer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#verl-trainer-configuration">veRL Trainer Configuration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="trinity_programming_guide.html">Trinity-RFT Developer Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../build_api/trinity.buffer.html">trinity.buffer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build_api/trinity.explorer.html">trinity.explorer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build_api/trinity.trainer.html">trinity.trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build_api/trinity.manager.html">trinity.manager</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build_api/trinity.common.html">trinity.common</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build_api/trinity.utils.html">trinity.utils</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Trinity-RFT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Trinity-RFT Configuration</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorial/trinity_configs.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="trinity-rft-configuration">
<h1>Trinity-RFT Configuration<a class="headerlink" href="#trinity-rft-configuration" title="Link to this heading"></a></h1>
<p>The following is the main config file for Trinity-RFT. Take <code class="docutils literal notranslate"><span class="pre">countdown.yaml</span></code> as an example.</p>
<section id="global-config">
<h2>Global Config<a class="headerlink" href="#global-config" title="Link to this heading"></a></h2>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">both</span>
<span class="nt">global_config</span><span class="p">:</span>
<span class="w">  </span><span class="nt">total_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">96</span>
<span class="w">  </span><span class="nt">eval_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
<span class="w">  </span><span class="nt">eval_on_latest_ckp</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode</span></code>: The mode of the experiment, chosen from <code class="docutils literal notranslate"><span class="pre">both</span></code>, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">explore</span></code> or <code class="docutils literal notranslate"><span class="pre">bench</span></code>. <code class="docutils literal notranslate"><span class="pre">both</span></code> means both trainer and explorer are launched; <code class="docutils literal notranslate"><span class="pre">train</span></code> means only trainer is launched; <code class="docutils literal notranslate"><span class="pre">explore</span></code> means only explorer is launched; <code class="docutils literal notranslate"><span class="pre">bench</span></code> conducts benchmark evaluation. Default is <code class="docutils literal notranslate"><span class="pre">both</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">global_config.total_epochs</span></code>: The total number of epochs. It should be checked manually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">global_config.batch_size</span></code>: The batch size used for training. It should be checked manually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">global_config.eval_interval</span></code>: The interval steps between two evaluations. Default is <code class="docutils literal notranslate"><span class="pre">1000</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">global_config.eval_on_latest_ckp</span></code>: Whether to evaluate on only the latest checkpoint or all the checkpoints in the path. Only valid in <code class="docutils literal notranslate"><span class="pre">bench</span></code> mode. Default is <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p></li>
</ul>
</section>
<section id="monitor">
<h2>Monitor<a class="headerlink" href="#monitor" title="Link to this heading"></a></h2>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">monitor</span><span class="p">:</span>
<span class="w">  </span><span class="nt">project</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Trinity-RFT-countdown&quot;</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;qwen2.5-1.5B-countdown&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">monitor.project</span></code>: The project name. It must be set manually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">monitor.name</span></code>: The name of the experiment. It must be set manually.</p></li>
</ul>
</section>
<section id="data-processing">
<h2>Data Processing<a class="headerlink" href="#data-processing" title="Link to this heading"></a></h2>
<!-- The `data` configuration specifies the data used for training. It includes the total number of epochs, the batch size, the path to the dataset, the default workflow type, the default reward function type, and the format configuration. -->
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">data_processor</span><span class="p">:</span>
<span class="w">  </span><span class="nt">source_data_path</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;/PATH/TO/DATASET&#39;</span>
<span class="w">  </span><span class="nt">load_kwargs</span><span class="p">:</span>
<span class="w">    </span><span class="nt">split</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;train&#39;</span><span class="w">  </span><span class="c1"># only need the train split</span>
<span class="w">  </span><span class="nt">format</span><span class="p">:</span>
<span class="w">    </span><span class="nt">prompt_key</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;question&#39;</span>
<span class="w">    </span><span class="nt">response_key</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;answer&#39;</span>

<span class="w">  </span><span class="c1"># cleaner related</span>
<span class="w">  </span><span class="nt">dj_config_path</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;tests/test_configs/active_iterator_test_dj_cfg.yaml&#39;</span>
<span class="w">  </span><span class="nt">clean_strategy</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;iterative&#39;</span>
<span class="w">  </span><span class="c1"># db related</span>
<span class="w">  </span><span class="nt">db_url</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;postgresql://{username}@localhost:5432/{db_name}&#39;</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">data.source_data_path</span></code>: The path to the source dataset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data.load_kwargs</span></code>: The kwargs used in <code class="docutils literal notranslate"><span class="pre">datasets.load_dataset</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data.format</span></code>: The format of the source dataset. It includes <code class="docutils literal notranslate"><span class="pre">prompt_key</span></code> and <code class="docutils literal notranslate"><span class="pre">response_key</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data.dj_config_path</span></code>: The path to the Data-Juicer configuration.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data.clean_strategy</span></code>: The cleaning strategy used for <code class="docutils literal notranslate"><span class="pre">DataCleaner</span></code>, which iteratively cleans dataset until targets are met.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data.db_url</span></code>: The URL of the database.</p></li>
</ul>
</section>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Link to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">model</span></code> configuration specifies the model used for training. It includes the path to the model checkpoint, the maximum number of tokens in the prompt, the maximum number of tokens in the response, the path to the checkpoint of the model, and whether to load the checkpoint of the model.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">model_path</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;/PATH/TO/MODEL/CHECKPOINT/&#39;</span>
<span class="w">  </span><span class="nt">critic_model_path</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;&#39;</span>
<span class="w">  </span><span class="nt">max_prompt_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="w">  </span><span class="nt">max_response_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1024</span>
<span class="w">  </span><span class="nt">checkpoint_path</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;checkpoints/qwen2.5-1.5B-countdown&#39;</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model.model_path</span></code>: The path to the model checkpoint. It must be set manually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model.critic_model_path</span></code>: The path to the critic model checkpoint. If not set, the <code class="docutils literal notranslate"><span class="pre">model.critic_model_path</span></code> will be set to <code class="docutils literal notranslate"><span class="pre">model.model_path</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model.max_prompt_tokens</span></code>: The maximum number of tokens in the prompt. Default is <code class="docutils literal notranslate"><span class="pre">2048</span></code>. It should be set manually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model.max_response_tokens</span></code>: The maximum number of tokens in the response. Default is <code class="docutils literal notranslate"><span class="pre">2048</span></code>. It should be set manually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model.checkpoint_path</span></code>: The path to the checkpoint of the model. It must be set manually.</p></li>
</ul>
</section>
<section id="cluster">
<h2>Cluster<a class="headerlink" href="#cluster" title="Link to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">cluster</span></code> configuration specifies the cluster configuration. It includes the number of nodes and the number of GPUs per node.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">cluster</span><span class="p">:</span>
<span class="w">  </span><span class="nt">node_num</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">gpu_per_node</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cluster.node_num</span></code>: The number of nodes used for training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cluster.gpu_per_node</span></code>: The number of GPUs per node used for training.</p></li>
</ul>
</section>
<section id="buffer">
<h2>Buffer<a class="headerlink" href="#buffer" title="Link to this heading"></a></h2>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">buffer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">max_retry_times</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="w">  </span><span class="nt">max_retry_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">explorer_input</span><span class="p">:</span>
<span class="w">    </span><span class="nt">taskset</span><span class="p">:</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">countdown</span>
<span class="w">      </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;countdown_dataset/oneshot-split&#39;</span>
<span class="w">      </span><span class="nt">split</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">train</span>
<span class="w">      </span><span class="nt">format</span><span class="p">:</span>
<span class="w">        </span><span class="nt">prompt_key</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;question&#39;</span>
<span class="w">        </span><span class="nt">response_key</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;answer&#39;</span>
<span class="w">    </span><span class="nt">eval_tasksets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="w">    </span><span class="nt">default_workflow_type</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;math_workflow&#39;</span>
<span class="w">    </span><span class="nt">default_reward_fn_type</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;countdown_reward&#39;</span>
<span class="w">  </span><span class="nt">trainer_input</span><span class="p">:</span>
<span class="w">    </span><span class="nt">experience_buffer</span><span class="p">:</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">countdown_buffer</span>
<span class="w">      </span><span class="nt">storage_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">queue</span>
<span class="w">      </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;sqlite:///countdown.db&#39;</span>
<span class="w">    </span><span class="nt">sft_warmup_dataset</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.max_retry_times</span></code>: The maximum number of retries when loading the data from database.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.max_retry_interval</span></code>: The maximum interval between retries when loading the data from database.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.explorer_input.taskset</span></code>: The configuration of the taskset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.explorer_input.taskset.name</span></code>: The name of the taskset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.explorer_input.taskset.path</span></code>: The path to the taskset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.explorer_input.taskset.split</span></code>: The split name of the taskset used for training. Default is <code class="docutils literal notranslate"><span class="pre">train</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.explorer_input.taskset.format</span></code>: The format of the taskset. It includes <code class="docutils literal notranslate"><span class="pre">prompt_key</span></code>, <code class="docutils literal notranslate"><span class="pre">response_key</span></code>, <code class="docutils literal notranslate"><span class="pre">workflow_key</span></code> and <code class="docutils literal notranslate"><span class="pre">reward_fn_key</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.explorer_input.eval_tasksets</span></code>: The configuration of the eval tasksets. It is a list of tasksets which will be used for evaluation. And it is empty by default.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.explorer_input.default_workflow_type</span></code>: The default workflow type for <code class="docutils literal notranslate"><span class="pre">taskset</span></code> and <code class="docutils literal notranslate"><span class="pre">eval_tasksets</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.explorer_input.default_reward_fn_type</span></code>: The default reward function type for <code class="docutils literal notranslate"><span class="pre">taskset</span></code> and <code class="docutils literal notranslate"><span class="pre">eval_tasksets</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.trainer_input.experience_buffer</span></code>: The configuration of experience_buffer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.trainer_input.experience_buffer.name</span></code>: The name of the experience buffer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.trainer_input.experience_buffer.storage_type</span></code>: The storage type of the experience buffer. Default is <code class="docutils literal notranslate"><span class="pre">queue</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.trainer_input.experience_buffer.path</span></code>: The sql path to store the experience buffer. It can be empty to indicate not saving to the database.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer.trainer_input.sft_warmup_dataset</span></code>: The configuration of the SFT warmup dataset. The structure of <code class="docutils literal notranslate"><span class="pre">sft_warmup_dataset</span></code> is the similar to <code class="docutils literal notranslate"><span class="pre">buffer.explorer_input.taskset</span></code>.</p></li>
</ul>
</section>
<section id="explorer">
<h2>Explorer<a class="headerlink" href="#explorer" title="Link to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">explorer</span></code> configuration specifies the explorer configuration. It includes the type of the engine, the number of engines, the number of workflow runners, the tensor parallel size, whether to enable prefix caching, whether to enforce eager mode, the data type, the <code class="docutils literal notranslate"><span class="pre">temperature</span></code>, the <code class="docutils literal notranslate"><span class="pre">top-p</span></code>, the <code class="docutils literal notranslate"><span class="pre">top-k</span></code>, the <code class="docutils literal notranslate"><span class="pre">seed</span></code>, the <code class="docutils literal notranslate"><span class="pre">logprobs</span></code>, the number of times to repeat each task, whether to use Ray, the backend, the maximum number of pending requests, and the maximum number of waitingsteps.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">explorer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">engine_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm_async</span>
<span class="w">  </span><span class="nt">engine_num</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">runner_num</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
<span class="w">  </span><span class="nt">tensor_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">enable_prefix_caching</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">  </span><span class="nt">enforce_eager</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">dtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bfloat16</span>
<span class="w">  </span><span class="nt">temperature</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">  </span><span class="nt">seed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">42</span>
<span class="w">  </span><span class="nt">logprobs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">  </span><span class="nt">repeat_times</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">  </span><span class="nt">use_ray</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">  </span><span class="nt">backend</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;nccl&#39;</span>
<span class="w">  </span><span class="nt">max_pending_requests</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32</span>
<span class="w">  </span><span class="nt">max_waiting_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.engine_type</span></code>: The type of the engine, Support <code class="docutils literal notranslate"><span class="pre">vllm_async</span></code> and <code class="docutils literal notranslate"><span class="pre">vllm_sync</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">vllm_async</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.engine_num</span></code>: The number of engines. Default is <code class="docutils literal notranslate"><span class="pre">2</span></code>. It should be set manually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.runner_num</span></code>: The number of workflow runners. Default is <code class="docutils literal notranslate"><span class="pre">32</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.tensor_parallel_size</span></code>: The tensor parallel size used in vLLM. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.enable_prefix_caching</span></code>: Whether to enable prefix caching. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.enforce_eager</span></code>: Whether to enforce eager mode. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.dtype</span></code>: The data type used in vLLM. Default is <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.temperature</span></code>: The temperature used in vLLM. Default is <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.seed</span></code>: The seed used in vLLM. Default is <code class="docutils literal notranslate"><span class="pre">42</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.logprobs</span></code>: The logprobs used in vLLM. Default is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.repeat_times</span></code>: The number of times to repeat each task, used for GRPO-like algorithms. Default is <code class="docutils literal notranslate"><span class="pre">5</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.use_ray</span></code>: Whether to use Ray. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.backend</span></code>: The backend used in vLLM. Default is <code class="docutils literal notranslate"><span class="pre">nccl</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.max_pending_requests</span></code>: The maximum number of pending requests. Default is <code class="docutils literal notranslate"><span class="pre">32</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">explorer.max_waiting_steps</span></code>: The maximum number of waiting steps. Default is <code class="docutils literal notranslate"><span class="pre">4</span></code>.</p></li>
</ul>
</section>
<section id="synchronizer">
<h2>Synchronizer<a class="headerlink" href="#synchronizer" title="Link to this heading"></a></h2>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">synchronizer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">sync_method</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;nccl&#39;</span>
<span class="w">  </span><span class="nt">sync_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<span class="w">  </span><span class="nt">sync_timeout</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1200</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">synchronizer.sync_method</span></code>: The synchronization method between <code class="docutils literal notranslate"><span class="pre">trainer</span></code> and <code class="docutils literal notranslate"><span class="pre">explorer</span></code>.
Support <code class="docutils literal notranslate"><span class="pre">nccl</span></code> and <code class="docutils literal notranslate"><span class="pre">checkpoint</span></code>, <code class="docutils literal notranslate"><span class="pre">nccl</span></code> represents that model weights in <code class="docutils literal notranslate"><span class="pre">explorer</span></code> will be synchronized from <code class="docutils literal notranslate"><span class="pre">trainer</span></code> through <code class="docutils literal notranslate"><span class="pre">nccl</span></code>,
<code class="docutils literal notranslate"><span class="pre">checkpoint</span></code> represents that <code class="docutils literal notranslate"><span class="pre">explorer</span></code> will load the newest checkpoints saved by <code class="docutils literal notranslate"><span class="pre">trainer</span></code> then update its model weights. Default is <code class="docutils literal notranslate"><span class="pre">nccl</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">synchronizer.sync_interval</span></code>: The interval steps between two synchronizations. Default is <code class="docutils literal notranslate"><span class="pre">10</span></code>. It should be set manually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">synchronizer.sync_timeout</span></code>: The timeout of the synchronization. Default is <code class="docutils literal notranslate"><span class="pre">1200</span></code>.</p></li>
</ul>
</section>
<section id="trainer">
<h2>Trainer<a class="headerlink" href="#trainer" title="Link to this heading"></a></h2>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">trainer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">trainer_type</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;verl&#39;</span>
<span class="w">  </span><span class="nt">algorithm_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ppo</span>
<span class="w">  </span><span class="nt">trainer_config_path</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;examples/ppo_countdown/train_countdown.yaml&#39;</span>
<span class="w">  </span><span class="nt">sft_warmup_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">  </span><span class="nt">eval_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
<span class="w">  </span><span class="nt">save_interval</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.trainer_type</span></code>: The backend of the trainer, Only <code class="docutils literal notranslate"><span class="pre">verl</span></code> is supported.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.algorithm_type</span></code>: The type of the algorithm, Support <code class="docutils literal notranslate"><span class="pre">ppo</span></code>, <code class="docutils literal notranslate"><span class="pre">grpo</span></code>, <code class="docutils literal notranslate"><span class="pre">opmd</span></code> and <code class="docutils literal notranslate"><span class="pre">dpo</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.trainer_config_path</span></code>: The path to the trainer configuration file. It must be set manually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.sft_warmup_steps</span></code>: The number of steps to warm up the model. Default is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.eval_interval</span></code>: The interval steps between two evaluations. Default is <code class="docutils literal notranslate"><span class="pre">1000</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.save_interval</span></code>: The interval steps between two checkpoints. Default is <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
</ul>
<section id="verl-trainer-configuration">
<h3>veRL Trainer Configuration<a class="headerlink" href="#verl-trainer-configuration" title="Link to this heading"></a></h3>
<p>Here we mainly introduce the parameters that can be set in veRL. For the specific meaning of the parameters, please refer to the official document of <a class="reference external" href="https://github.com/volcengine/verl/blob/0bdf7f469854815177e73dcfe9e420836c952e6e/docs/examples/config.rst">veRL</a>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">data</span><span class="p">:</span>
<span class="w">  </span><span class="nt">tokenizer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">  </span><span class="nt">train_files</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">train_example.parquet</span>
<span class="w">  </span><span class="nt">val_files</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">test_example.parquet</span>
<span class="w">  </span><span class="nt">prompt_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">prompt</span>
<span class="w">  </span><span class="nt">max_prompt_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="w">  </span><span class="nt">max_response_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1024</span>
<span class="w">  </span><span class="nt">train_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
<span class="w">  </span><span class="nt">val_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">  </span><span class="nt">return_raw_input_ids</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span><span class="w">  </span><span class="c1"># This should be set to true when the tokenizer between policy and rm differs</span>
<span class="w">  </span><span class="nt">return_raw_chat</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">shuffle</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">  </span><span class="nt">filter_overlong_prompts</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span><span class="w"> </span><span class="c1"># for large-scale dataset, filtering overlong prompts could be timeconsuming. You should disable this and set `truncation=&#39;left&#39;</span>
<span class="w">  </span><span class="nt">truncation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">error</span>
<span class="w">  </span><span class="nt">image_key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">images</span>

<span class="nt">actor_rollout_ref</span><span class="p">:</span>
<span class="w">  </span><span class="nt">hybrid_engine</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">  </span><span class="nt">model</span><span class="p">:</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/PATH/TO/MODEL/CHECKPOINT/</span>
<span class="w">    </span><span class="nt">external_lib</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">override_config</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{</span><span class="w"> </span><span class="p p-Indicator">}</span>
<span class="w">    </span><span class="nt">enable_gradient_checkpointing</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="nt">use_remove_padding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">  </span><span class="nt">actor</span><span class="p">:</span>
<span class="w">    </span><span class="nt">strategy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fsdp</span><span class="w">  </span><span class="c1"># This is for backward-compatibility</span>
<span class="w">    </span><span class="nt">ppo_mini_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span>
<span class="w">    </span><span class="c1"># ppo_micro_batch_size: 8 # will be deprecated, use ppo_micro_batch_size_per_gpu</span>
<span class="w">    </span><span class="nt">ppo_micro_batch_size_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">    </span><span class="nt">use_dynamic_bsz</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="nt">ppo_max_token_len_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16384</span><span class="w"> </span><span class="c1"># n * ${data.max_prompt_length} + ${data.max_response_length}</span>
<span class="w">    </span><span class="nt">grad_clip</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">clip_ratio</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.2</span>
<span class="w">    </span><span class="nt">entropy_coeff</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.001</span>
<span class="w">    </span><span class="nt">use_kl_loss</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span><span class="w"> </span><span class="c1"># True for GRPO</span>
<span class="w">    </span><span class="nt">kl_loss_coef</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.001</span><span class="w"> </span><span class="c1"># for grpo</span>
<span class="w">    </span><span class="nt">kl_loss_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">low_var_kl</span><span class="w"> </span><span class="c1"># for grpo</span>
<span class="w">    </span><span class="nt">ppo_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">shuffle</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">    </span><span class="nt">ulysses_sequence_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"> </span><span class="c1"># sp size</span>
<span class="w">    </span><span class="nt">checkpoint</span><span class="p">:</span>
<span class="w">      </span><span class="nt">contents</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&#39;model&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;hf_model&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;optimizer&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;extra&#39;</span><span class="p p-Indicator">]</span><span class="w">  </span><span class="c1"># with &#39;hf_model&#39; you can save whole model as hf format, now only use sharded model checkpoint to save space</span>
<span class="w">    </span><span class="nt">optim</span><span class="p">:</span>
<span class="w">      </span><span class="nt">lr</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-6</span>
<span class="w">      </span><span class="nt">lr_warmup_steps_ratio</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.</span><span class="w">  </span><span class="c1"># the total steps will be injected during runtime</span>
<span class="w">      </span><span class="c1"># min_lr_ratio: null   # only useful for warmup with cosine</span>
<span class="w">      </span><span class="nt">warmup_style</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">constant</span><span class="w">  </span><span class="c1"># select from constant/cosine</span>
<span class="w">      </span><span class="nt">total_training_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span><span class="w">  </span><span class="c1"># must be override by program</span>
<span class="w">    </span><span class="nt">fsdp_config</span><span class="p">:</span>
<span class="w">      </span><span class="nt">wrap_policy</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># transformer_layer_cls_to_wrap: None</span>
<span class="w">        </span><span class="nt">min_num_params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">      </span><span class="nt">param_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">      </span><span class="nt">optimizer_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">      </span><span class="nt">fsdp_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span>
<span class="w">    </span><span class="c1"># --- below: opmd ---</span>
<span class="w">    </span><span class="nt">tau</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.000</span><span class="w">  </span><span class="c1"># strength of regularization w.r.t. old / ref policy</span>
<span class="w">    </span><span class="nt">opmd_baseline</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mean</span><span class="w">  </span><span class="c1"># mean / logavgexp, applicable to opmd</span>
<span class="w">    </span><span class="nt">use_uid</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span><span class="w">  </span><span class="c1"># True / False, applicable to pairwise_opmd</span>
<span class="w">  </span><span class="nt">ref</span><span class="p">:</span>
<span class="w">    </span><span class="nt">fsdp_config</span><span class="p">:</span>
<span class="w">      </span><span class="nt">param_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">      </span><span class="nt">wrap_policy</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># transformer_layer_cls_to_wrap: None</span>
<span class="w">        </span><span class="nt">min_num_params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">    </span><span class="c1"># log_prob_micro_batch_size: 4 # will be deprecated, use log_prob_micro_batch_size_per_gpu</span>
<span class="w">    </span><span class="nt">log_prob_micro_batch_size_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">    </span><span class="nt">log_prob_use_dynamic_bsz</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.actor.use_dynamic_bsz}</span>
<span class="w">    </span><span class="nt">log_prob_max_token_len_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}</span>
<span class="w">    </span><span class="nt">ulysses_sequence_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.actor.ulysses_sequence_parallel_size}</span><span class="w"> </span><span class="c1"># sp size</span>
<span class="w">  </span><span class="nt">rollout</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm</span>
<span class="w">    </span><span class="nt">temperature</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">    </span><span class="nt">top_k</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span><span class="w"> </span><span class="c1"># 0 for hf rollout, -1 for vllm rollout</span>
<span class="w">    </span><span class="nt">top_p</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">    </span><span class="nt">use_fire_sampling</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span><span class="w"> </span><span class="c1"># https://arxiv.org/abs/2410.21236</span>
<span class="w">    </span><span class="nt">prompt_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${data.max_prompt_length}</span><span class="w">  </span><span class="c1"># not use for opensource</span>
<span class="w">    </span><span class="nt">response_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${data.max_response_length}</span>
<span class="w">    </span><span class="c1"># for vllm rollout</span>
<span class="w">    </span><span class="nt">dtype</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bfloat16</span><span class="w"> </span><span class="c1"># should align with FSDP</span>
<span class="w">    </span><span class="nt">gpu_memory_utilization</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.4</span>
<span class="w">    </span><span class="nt">ignore_eos</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">    </span><span class="nt">enforce_eager</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="nt">free_cache_engine</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="nt">load_format</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">dummy_dtensor</span>
<span class="w">    </span><span class="nt">tensor_model_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">    </span><span class="nt">max_num_batched_tokens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8192</span>
<span class="w">    </span><span class="nt">max_model_len</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">    </span><span class="nt">max_num_seqs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1024</span>
<span class="w">    </span><span class="c1"># log_prob_micro_batch_size: 8 # will be deprecated, use log_prob_micro_batch_size_per_gpu</span>
<span class="w">    </span><span class="nt">log_prob_micro_batch_size_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="w">    </span><span class="nt">log_prob_use_dynamic_bsz</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.actor.use_dynamic_bsz}</span>
<span class="w">    </span><span class="nt">log_prob_max_token_len_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}</span>
<span class="w">    </span><span class="nt">disable_log_stats</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="nt">enable_chunked_prefill</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"> </span><span class="c1"># could get higher throughput</span>
<span class="w">    </span><span class="c1"># for hf rollout</span>
<span class="w">    </span><span class="nt">do_sample</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="c1"># number of responses (i.e. num sample times)</span>
<span class="w">    </span><span class="nt">n</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"> </span><span class="c1"># &gt; 1 for grpo</span>

<span class="nt">critic</span><span class="p">:</span>
<span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fsdp</span>
<span class="w">  </span><span class="nt">optim</span><span class="p">:</span>
<span class="w">    </span><span class="nt">lr</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1e-5</span>
<span class="w">    </span><span class="nt">lr_warmup_steps_ratio</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.</span><span class="w">  </span><span class="c1"># the total steps will be injected during runtime</span>
<span class="w">    </span><span class="c1"># min_lr_ratio: null   # only useful for warmup with cosine</span>
<span class="w">    </span><span class="nt">warmup_style</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">constant</span><span class="w">  </span><span class="c1"># select from constant/cosine</span>
<span class="w">    </span><span class="nt">total_training_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span><span class="w">  </span><span class="c1"># must be override by program</span>
<span class="w">  </span><span class="nt">model</span><span class="p">:</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/PATH/TO/MODEL/CHECKPOINT/</span>
<span class="w">    </span><span class="nt">tokenizer_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.model.path}</span>
<span class="w">    </span><span class="nt">override_config</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{</span><span class="w"> </span><span class="p p-Indicator">}</span>
<span class="w">    </span><span class="nt">external_lib</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.model.external_lib}</span>
<span class="w">    </span><span class="nt">enable_gradient_checkpointing</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">    </span><span class="nt">use_remove_padding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">    </span><span class="nt">fsdp_config</span><span class="p">:</span>
<span class="w">      </span><span class="nt">param_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">      </span><span class="nt">optimizer_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">      </span><span class="nt">wrap_policy</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># transformer_layer_cls_to_wrap: None</span>
<span class="w">        </span><span class="nt">min_num_params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">      </span><span class="nt">fsdp_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span>
<span class="w">  </span><span class="nt">ppo_mini_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.actor.ppo_mini_batch_size}</span>
<span class="w">  </span><span class="c1"># ppo_micro_batch_size: 8 # will be deprecated, use ppo_micro_batch_size_per_gpu</span>
<span class="w">  </span><span class="nt">ppo_micro_batch_size_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">  </span><span class="nt">forward_micro_batch_size_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${critic.ppo_micro_batch_size_per_gpu}</span>
<span class="w">  </span><span class="nt">use_dynamic_bsz</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.actor.use_dynamic_bsz}</span>
<span class="w">  </span><span class="nt">ppo_max_token_len_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">32768</span><span class="w"> </span><span class="c1"># (${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}) * 2</span>
<span class="w">  </span><span class="nt">forward_max_token_len_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${critic.ppo_max_token_len_per_gpu}</span>
<span class="w">  </span><span class="nt">ulysses_sequence_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"> </span><span class="c1"># sp size</span>
<span class="w">  </span><span class="nt">ppo_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.actor.ppo_epochs}</span>
<span class="w">  </span><span class="nt">shuffle</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.actor.shuffle}</span>
<span class="w">  </span><span class="nt">grad_clip</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">  </span><span class="nt">cliprange_value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5</span>

<span class="nt">reward_model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">enable</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">strategy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fsdp</span>
<span class="w">  </span><span class="nt">model</span><span class="p">:</span>
<span class="w">    </span><span class="nt">input_tokenizer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.model.path}</span><span class="w">  </span><span class="c1"># set this to null if the chat template is identical</span>
<span class="w">    </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">~/models/FsfairX-LLaMA3-RM-v0.1</span>
<span class="w">    </span><span class="nt">external_lib</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${actor_rollout_ref.model.external_lib}</span>
<span class="w">    </span><span class="nt">use_remove_padding</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">    </span><span class="nt">fsdp_config</span><span class="p">:</span>
<span class="w">      </span><span class="nt">min_num_params</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">      </span><span class="nt">param_offload</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">      </span><span class="nt">fsdp_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1</span>
<span class="w">  </span><span class="c1"># micro_batch_size: null # will be deprecated, use micro_batch_size_per_gpu</span>
<span class="w">  </span><span class="c1"># micro_batch_size_per_gpu: 2 # set a number</span>
<span class="w">  </span><span class="c1"># max_length: null</span>
<span class="w">  </span><span class="nt">ulysses_sequence_parallel_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"> </span><span class="c1"># sp size</span>
<span class="w">  </span><span class="nt">use_dynamic_bsz</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${critic.use_dynamic_bsz}</span>
<span class="w">  </span><span class="nt">forward_max_token_len_per_gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${critic.forward_max_token_len_per_gpu}</span>
<span class="w">  </span><span class="nt">reward_manager</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tinyzero</span>

<span class="nt">custom_reward_function</span><span class="p">:</span>
<span class="w">  </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">compute_score</span>

<span class="nt">algorithm</span><span class="p">:</span>
<span class="w">  </span><span class="nt">gamma</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">  </span><span class="nt">lam</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
<span class="w">  </span><span class="nt">adv_estimator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gae</span>
<span class="w">  </span><span class="nt">norm_adv_by_std_in_grpo</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">  </span><span class="nt">use_kl_in_reward</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">kl_penalty</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kl</span><span class="w">  </span><span class="c1"># how to estimate kl divergence</span>
<span class="w">  </span><span class="nt">kl_ctrl</span><span class="p">:</span>
<span class="w">    </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">fixed</span>
<span class="w">    </span><span class="nt">kl_coef</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.001</span>
<span class="w">    </span><span class="nt">horizon</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10000</span>
<span class="w">    </span><span class="nt">target_kl</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span>

<span class="nt">trainer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">balance_batch</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>
<span class="w">  </span><span class="nt">total_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15</span>
<span class="w">  </span><span class="c1"># total_training_steps: null</span>
<span class="w">  </span><span class="nt">project_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TinyZero</span>
<span class="w">  </span><span class="nt">experiment_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">trinity-qwen2.5-1.5b</span>
<span class="w">  </span><span class="nt">logger</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="s">&#39;wandb&#39;</span><span class="w"> </span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">val_generations_to_log_to_wandb</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">  </span><span class="nt">nnodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">n_gpus_per_node</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">save_freq</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">  </span><span class="c1"># auto: find the last ckpt to resume. If can&#39;t find, start from scratch</span>
<span class="w">  </span><span class="nt">resume_mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">auto</span><span class="w"> </span><span class="c1"># or auto or resume_path if</span>
<span class="w">  </span><span class="nt">resume_from_path</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span>
<span class="w">  </span><span class="nt">test_freq</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">  </span><span class="nt">critic_warmup</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">  </span><span class="nt">default_hdfs_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">  </span><span class="nt">remove_previous_ckpt_in_save</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">del_local_ckpt_after_load</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">default_local_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">checkpoints/${trainer.project_name}/${trainer.experiment_name}</span>
<span class="w">  </span><span class="nt">val_before_train</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
<span class="w">  </span><span class="nt">max_actor_ckpt_to_keep</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">  </span><span class="nt">max_critic_ckpt_to_keep</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.model.enable_gradient_checkpointing</span></code>: Whether to enable gradient checkpointing, which will reduce GPU memory usage.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.model.use_remove_padding</span></code>: Whether to remove pad tokens, which will reduce training time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.use_dynamic_bsz</span></code>: Whether to reorganize the batch data, specifically to splice the shorter data to reduce the batch size in the actual training process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu</span></code>: Batch size for one GPU in one forward pass.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.grad_clip</span></code>: Gradient clip for actor model training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.clip_ratio</span></code>: Used for compute policy loss.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.entropy_coeff</span></code>: Used for compute policy loss.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.use_kl_loss</span></code>: Whether to enable kl loss.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.kl_loss_coef</span></code>: The coefficient of kl loss.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.kl_loss_type</span></code>: How to compute kl loss, optional value is <code class="docutils literal notranslate"><span class="pre">kl</span></code>, <code class="docutils literal notranslate"><span class="pre">abs</span></code>, <code class="docutils literal notranslate"><span class="pre">mse</span></code> or <code class="docutils literal notranslate"><span class="pre">low_var_kl</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.ulysses_sequence_parallel_size</span></code>: Ulysses sequence parallel size.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.tau</span></code>: strength of regularization w.r.t. old / ref policy.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.opmd_baseline</span></code>: mean / logavgexp, applicable to opmd.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.use_uid</span></code>: True / False, applicable to pairwise_opmd.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.optim.lr</span></code>: Learning rate for actor model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.optim.lr_warmup_steps_ratio</span></code>: Ratio of warmup steps for learning rate.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.optim.warmup_style</span></code>: Warmup style for learning rate.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.actor.optim.total_training_steps</span></code>: Total training steps for actor model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu</span></code>: Batch size for one GPU in one reference model forward pass.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic.model.enable_gradient_checkpointing</span></code>: Whether to enable gradient checkpointing, which will reduce GPU memory usage.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic.model.use_remove_padding</span></code>: Whether to remove pad tokens, which will reduce training time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic.optim.lr</span></code>: Learning rate for critic model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic.optim.lr_warmup_steps_ratio</span></code>: Ratio of warmup steps for learning rate.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic.optim.warmup_style</span></code>: Warmup style for learning rate.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic.optim.total_training_steps</span></code>: Total training steps for critic model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic.ppo_micro_batch_size_per_gpu</span></code>: Batch size for one GPU in one critic model forward pass.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic.ulysses_sequence_parallel_size</span></code>: Ulysses sequence parallel size.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic.grad_clip</span></code>: Gradient clip for critic model training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">critic.cliprange_value</span></code>: Used for compute value loss.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">algorithm</span></code>: Training algorithm settings.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.balance_batch</span></code>: Whether to balance batch size between GPUs during training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.resume_mode</span></code>: Resume mode for training. Support <code class="docutils literal notranslate"><span class="pre">disable</span></code>, <code class="docutils literal notranslate"><span class="pre">auto</span></code> and <code class="docutils literal notranslate"><span class="pre">resume_path</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.resume_from_path</span></code>: Path to resume from.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.critic_warmup</span></code>: The number of steps to train the critic model before actual policy learning.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.default_hdfs_dir</span></code>: Default HDFS directory for saving checkpoints.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.remove_previous_ckpt_in_save</span></code>: Whether to remove previous checkpoints in save.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.del_local_ckpt_after_load</span></code>: Whether to delete local checkpoints after loading.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.max_actor_ckpt_to_keep</span></code>: Maximum number of actor checkpoints to keep.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainer.max_critic_ckpt_to_keep</span></code>: Maximum number of critic checkpoints to keep.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="example_data_functionalities.html" class="btn btn-neutral float-left" title="Data processing functionalities" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="trinity_programming_guide.html" class="btn btn-neutral float-right" title="Trinity-RFT Developer Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Trinity-RFT Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>